---
title: " Logistic Q-Learning "
abstract: " We propose a new reinforcement learning algorithm derived from a regularized
  linear-programming formulation of optimal control in MDPs. The method is closely
  related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters
  et al. (2010), with the key difference that our method introduces a Q-function that
  enables efficient exact model-free implementation. The main feature of our algorithm
  (called QREPS) is a convex loss function for policy evaluation that serves as a
  theoretically sound alternative to the widely used squared Bellman error. We provide
  a practical saddle-point optimization method for minimizing this loss function and
  provide an error-propagation analysis that relates the quality of the individual
  updates to the performance of the output policy. Finally, we demonstrate the effectiveness
  of our method on a range of benchmark problems. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bas-serrano21a
month: 0
tex_title: " Logistic Q-Learning "
firstpage: 3610
lastpage: 3618
page: 3610-3618
order: 3610
cycles: false
bibtex_author: Bas-Serrano, Joan and Curi, Sebastian and Krause, Andreas and Neu,
  Gergely
author:
- given: Joan
  family: Bas-Serrano
- given: Sebastian
  family: Curi
- given: Andreas
  family: Krause
- given: Gergely
  family: Neu
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/bas-serrano21a/bas-serrano21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/bas-serrano21a/bas-serrano21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
